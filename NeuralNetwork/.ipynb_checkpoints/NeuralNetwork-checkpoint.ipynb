{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing single layer neural network for handwritten-digit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize # for minimizing gradient\n",
    "from scipy.io import loadmat # for loading .mat file\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding turns a class label n (out of k classes) into a vector of length k where index n is \"hot\" (1) while the rest are zero.<br>\n",
    "Eg. for digits labelled 1-10 (i.e. 0-9), 1 maybe represented as [0,1,0,0,0,0,0,0,0,0] (i.e it belongs to class 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat('ex3data1.mat')\n",
    "# data\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One-hot encoding the y dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 10),\n",
       " array([6], dtype=uint8),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False) \n",
    "# sparse returns Sparse matrix when True, else returns an array\n",
    "\n",
    "Yonehot = encoder.fit_transform(y)\n",
    "Yonehot.shape, y[3100], Yonehot[3100,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining the functions to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): # Sigmoid function for calculating chance of being that solution\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forwardProp(X, theta1, theta2):    # for forward propagating in the neural network towards final outccome\n",
    "    m = X.shape[0]\n",
    "    # adding the bias node\n",
    "    a1 = np.insert(X, obj=0, values=np.ones(m), axis=1)\n",
    "    z2 = a1 * theta1.T   # matrix multiplication\n",
    "    a2 = np.insert(sigmoid(z2), obj=0, values=np.ones(m), axis=1) # for hidden layer, adding bias node\n",
    "    z3 = a2 * theta2.T   # matrix multiplication\n",
    "    h = sigmoid(z3) # final hypothesis for layer 3 (final layer)\n",
    "    \n",
    "    return a1, z2, a2, z3, h\n",
    "\n",
    "\n",
    "def cost(params, inputSize, hiddenlayerSize, numOfLabels, X, y, rate):    # defining the cost function\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    \n",
    "    # Changing (reshaping) the parameter array into theta matrices depending on size of each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hiddenlayerSize * (inputSize + 1)], (hiddenlayerSize, (inputSize + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hiddenlayerSize * (inputSize + 1):],(numOfLabels, (hiddenlayerSize + 1))))\n",
    "    # since if a network has s(j) units in layer j \n",
    "    # and s(j+1) in layer j+1,\n",
    "    # then theta(j) will have dimension: s(j+1) x s(j)+1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # passing theta to feed-forward algorithm\n",
    "    a1, z2, a2, z3, h = forwardProp(X, theta1, theta2)\n",
    "    \n",
    "    # Computing the cost now (without regularization)\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        first = np.multiply(-y[i,:], np.log(h[i,:]))\n",
    "        second = np.multiply((1-y[i,:]), np.log(1 - h[i,:]))\n",
    "        J += np.sum((first - second))\n",
    "        \n",
    "    J = J/m\n",
    "    \n",
    "    # Regularizing the cost\n",
    "    J += (float(rate) /(2*m))*(np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying out the Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputSize = 400 # 400 points for each image of digit\n",
    "hiddenlayerSize = 25\n",
    "numOfLabels = 10\n",
    "rate = 1\n",
    "\n",
    "# randomly initializing the params array of the size of all parameters of network\n",
    "params = (np.random.random(size=hiddenlayerSize * (inputSize+1) + (numOfLabels*(hiddenlayerSize + 1))) - 0.5)*0.25\n",
    "\n",
    "m = X.shape[0]\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "\n",
    "theta1 = np.matrix(np.reshape(params[:hiddenlayerSize * (inputSize + 1)], (hiddenlayerSize, (inputSize + 1))))\n",
    "theta2 = np.matrix(np.reshape(params[hiddenlayerSize * (inputSize + 1):],(numOfLabels, (hiddenlayerSize + 1))))\n",
    "\n",
    "theta1.shape, theta2.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking out shapes to confirm if all have proper dimensions or not and then calculating cost using random values of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 25), (5000, 26), (5000, 10), (5000, 10))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, z2, a2, z3, h = forwardProp(X, theta1, theta2)\n",
    "a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.095865505430175"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(params, inputSize, hiddenlayerSize, numOfLabels, X, Yonehot, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions defining the sigmoid-gradient and Backproportionate Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigGradient(z):\n",
    "    return np.multiply(sigmoid(z), (1- sigmoid(z)))\n",
    "\n",
    "\n",
    "def backProp(params, inputSize, hiddenlayerSize, numOfLabels, X, y, rate): # For minimizing cost function\n",
    "    # starting is same as cost function\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    \n",
    "    # Changing (reshaping) the parameter array into theta matrices depending on size of each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hiddenlayerSize * (inputSize + 1)], (hiddenlayerSize, (inputSize + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hiddenlayerSize * (inputSize + 1):],(numOfLabels, (hiddenlayerSize + 1))))\n",
    "    \n",
    "    a1, z2, a2, z3, h = forwardProp(X, theta1, theta2)\n",
    "    \n",
    "    # Here starts the BackProp Algo:\n",
    "    J = 0\n",
    "    delta1 = np.zeros(theta1.shape) # (25, 401)\n",
    "    delta2 = np.zeros(theta2.shape) # (10, 26) \n",
    "    \n",
    "    # Compute the cost\n",
    "    for i in range(m):\n",
    "        first = np.multiply(-y[i,:], np.log(h[i,:]))\n",
    "        second = np.multiply((1-y[i,:]), np.log(1 - h[i,:]))\n",
    "        J += np.sum((first - second))\n",
    "        \n",
    "    J = J/m\n",
    "    \n",
    "    # Regularizing the cost\n",
    "    J += (float(rate) /(2*m))*(np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
    "    \n",
    "    \n",
    "    # performing backpropagation\n",
    "    for j in range(m):\n",
    "        a1j = a1[j,:] # (1, 401) -taking each row at a time\n",
    "        z2j = z2[j,:] # (1, 25)\n",
    "        a2j = a2[j,:] # (1, 26)\n",
    "        hj = h[j,:]   # (1, 10)\n",
    "        yj = y[j,:]   # (1, 10)\n",
    "        \n",
    "        d3j = hj -yj  # (1, 10) -difference between real and predicted value\n",
    "        \n",
    "        z2j = np.insert(z2j, obj=0, values=np.ones(1)) # (1, 26)\n",
    "        d2j = np.multiply((theta2.T * d3j.T).T, sigGradient(z2j)) # (1, 26)\n",
    "        \n",
    "        delta1 = delta1 + (d2j[:, 1:]).T * a1j\n",
    "        delta2 = delta2 + (d3j.T * a2j)\n",
    "        \n",
    "    delta1 = delta1/m\n",
    "    delta2 = delta2/m\n",
    "    \n",
    "    # Regularizing the term\n",
    "    delta1[:, 1:] = delta1[:, 1:] + (theta1[:, 1:] * rate)/m\n",
    "    delta2[:, 1:] = delta2[:, 1:] + (theta2[:, 1:] * rate)/m\n",
    "    \n",
    "    #flatten these matrices for passing into minimize function\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2))) # double brackets coz of more than 1 arguments\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "# IT'S OVER..IT'S FINALLY OVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calling the backprop algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.095865505430175, (10285,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J ,grad = backProp(params, inputSize, hiddenlayerSize, numOfLabels, X, Yonehot, rate)\n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now, training our model using minimize function from scipy library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.3392923647518514\n",
       "     jac: array([-1.29326227e-03,  7.94553313e-07,  1.00721862e-06, ...,\n",
       "        2.64325504e-04,  4.95264211e-05,  6.26773995e-05])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 24\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 0.32947015,  0.00397277,  0.00503609, ...,  0.98715763,\n",
       "       -2.01915746, -1.8178498 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin = minimize(fun=backProp, x0=params, args=(inputSize, hiddenlayerSize, numOfLabels, X, Yonehot, rate), \n",
    "               method='TNC', jac=True, options={'maxiter':250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predicting the outcome using the optimized value of parameters got from last step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [10],\n",
       "       [10],\n",
       "       ...,\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix(X)\n",
    "    \n",
    "    \n",
    "# Changing (reshaping) the parameter array into theta matrices depending on size of each layer\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hiddenlayerSize * (inputSize + 1)], (hiddenlayerSize, (inputSize + 1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hiddenlayerSize * (inputSize + 1):],(numOfLabels, (hiddenlayerSize + 1))))\n",
    "    \n",
    "a1, z2, a2, z3, h = forwardProp(X, theta1, theta2)\n",
    "Ypredict =np.array(np.argmax(h, axis=1) + 1)\n",
    "Ypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculating the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.11999999999999%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(Ypredict, y)]  \n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))  \n",
    "print('accuracy = {0}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
